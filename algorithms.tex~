\chapter{Algorithms and running time analysis} 
\label{cha:runn-time-analys}



An \emph{algorithm} executes a set of \emph{instructions}
used in common programming languages like  arithmetic
operations, comparisons or read/write instructions. The sequence of these instructions is controlled by \emph{loops and conditionals} like {\tt if, while, for} etc. 

Each of these instructions requires \emph{time}. The \emph{running time} of an algorithm is the \emph{number of instructions} that the algorithm performs. This number depends on the \emph{input} of the algorithm. 


\begin{example}
\label{ex-a-1}
  Consider the following algorithm to compute the product of two $n × n$ matrices. 
    \begin{tabbing}            
    {\bf for} \= $i=1,\dots,n$ \\ 
              \> {\bf for} \= $j=1,\dots,n$ \\
              \> \> $c_{ij} := 0$ \\
              \> \> {\bf for} \= $k=1,\dots,n$ \\
              \> \> \> $c_{ij} := c_{ij} + a_{ik} ⋅ a_{kj}$
            \end{tabbing} 

 The number of additions is $n^2 ⋅ (n-1)$ and the number of multiplications is $n^3$.
 The number of store-instructions is $n^2 ⋅(n+1)$. The number of read-instructions is of similar magnitude. 
\end{example}


The above example shows that an \emph{exact counting} is sometimes
tedious. Looking at the algorithm however, you quickly agree that
there exists \emph{some constant} $c \in \R_{>0}$ such that the
algorithm performs at most $c \cdot m \cdot n \cdot l$ instructions. 

In the analysis of algorithms, one does usually not care so much about the constant $c$ above in the beginning. There are sub-fields of algorithms where this constant however matters. Especially for algorithms on large data sets, where access to external data is costly. However, this is another story that does not concern us here. When we analyze algorithms, we are interested in the \emph{asymptotic running time}. 

\begin{definition}[$O$-notation]  

 Let $T,f: \setN \to \setR_{\geq0}$ be functions. We say 
  \begin{itemize}
  \item \emph{$T(n) = O(f(n))$}, if there exist positive  constants
    $n_o\in \setN$ and  $c\in\setR_{>0}$ 
    with $$T(n) \leq c \cdot f(n) \text{ for all }n\geq n_0.$$  
  \item \emph{$T(n) = \Omega(f(n))$}, if there exist constants   $n_o\in \setN$
    and  $c\in\setR_{>0}$ 
    with $$T(n) \geq c \cdot f(n)\text{ for all } n\geq n_0.$$
  \item \emph{$T(n) = \Theta(f(n))$}  if $$T(n)=O(f(n))\text{ and }
     T(n) = \Omega(f(n)).$$
  \end{itemize}
\end{definition}


\begin{example}
  The function $T(n)=2n^2 + 3n +1$ is in $O(n^2)$, since for all
  $n\geq1$ one has $2n^2 + 3n + 1 \leq 6n^2$. Here $n_0 = 1$ and $c =
  6$.  Similarly $T(n) = \Omega(n^2)$, since for each $n\geq1$ one has $2n^2 + 3n
  +1\geq n^2$. Thus $T(n)$ is in $\Theta(n^2)$. 
\end{example}

We measure the running time of algorithms in terms of the \emph{length
  of the input}.  The matrices $A$ and $B $ that are the input of the matrix-multiplication
algorithm of Example~\ref{ex-a-1} consist of $n^2$ 
numbers each. The algorithm runs in time $O(n^3 = (n^2)^{3/2})$. 


What does it mean for an algorithm to be efficient? For us, this will
mean that it runs in polynomial time. As a first definition of
polynomial time algorithm, we say that an algorithm runs in
\emph{polynomial time}, if there exists a constant $k$ such that the
algorithm runs in time $O(n^k)$, where $n$ is the length of the input
of the algorithm.

However, we recall the \emph{binary} representation of natural numbers $n ∈ ℕ$. A sequence of bits $a_0,\dots,a_{k-1}$ with $a_j ∈ \{0,1\}$ for $0 ≤j≤k-1$ represents the number 
\begin{displaymath}
  ∑_{j=0}^{k-1} a_j ⋅ 2^j. 
\end{displaymath}  
Conversely, each positive natural number $n≥1$ has the binary representation that is found recursively by the following process. 
If $n=1$, then its representation is $a_0 = 1$. 
If $n>1$ and  is even, then the sequence representing $n$ is 
\begin{displaymath}
  0,b_0,\dots,b_{k-1},
\end{displaymath}
where $b_0,\dots,b_{k-1}$ is the representation of $n/2$. If $n>1$ and n is odd, then the sequence representing $n$ is 
\begin{displaymath}
  1,b_0,\dots,b_{k-1},
\end{displaymath}
where $b_0,\dots,b_{k-1}$ is the representation of $⌊n/2⌋$. This creates a representation with leading bit one, i.e. $a_{k-1}=1$. 
By deleting \emph{leading zeros}, i.e., ensuring $a_{k-1}=1$, the representation of a natural positive number is unique, (see exercise~\ref{cha:runn-time-analys}.\ref{alg:ex2}). 



\begin{example}
  This example will make us revise the our first definition of a polynomial-time algorithm. 
The input of this algorithm is a list of characters. Let us say that the list has $n$ elements. 
 
 \label{ex-a-3}
  \begin{tabbing}
    Input: A list $L$ or characters \\
    $s :=2$ \\
    {\bf for} \= $c ∈ L$:\\
              \> $s:= s ⋅ s$ \\
    {\bf return} $s$         
  \end{tabbing}
  Then clearly, the algorithm carries out a polynomial, even linear,
  number of operations in the input, if we consider an arithmetic
  operation also as a basic operation that can be carried out in
  constant time. However, the algorithm squares $2$
  repeatedly, $n$
  times to be precise. Thus, at the end, the variable $s$
  holds the number $2^{2^n}$.
  The number of \emph{bits} in the binary representation  that the return value
  is $2^n$
  which is \emph{exponential} in the input length. 
\end{example}

\begin{definition}
  \label{def:2}
  The \emph{size} of an integer $x$
  is $\size(x) = \lceil\log( |x| +1)\rceil$
  and for $x \in \setQ$,
  $\size(x) = \size(p)+\size(q)$,
  where $x = p/q$ with $p,q \in \setZ$, $q\geq1$ and $\gcd(p,q)=1$. 
\end{definition}
Thus the size of a number is asymptotically equal to the number of
bits that are needed to store the number.  We now provide the
definition of what a polynomial-time algorithm is.
\begin{definition}
  \label{def:1}
  An  is \emph{polynomial time}, if there exists a
  constant $k$
  such that the algorithm performs $O(n^k)$
  operations on rational numbers whose size is bounded by
  $O(n^k)$.
  Here $n$
  is the number of bits that encode the input of the algorithm. We say that the algorithm runs in time $O(n^k)$. 
\end{definition}

We now use this definition to analyze the famous \emph{Euclidean} algorithm that computes the \emph{greatest common divisor} of two integers. 

 For $a,b \in \setZ$, $b \neq0$ we say $b$ \emph{divides} $a$ if there
    exists an $x \in \setZ$ such that $a = b \cdot  x$. We 
    write    $b\mid a$.  
   For $a,b,c \in \setZ$, if $c\mid a$ and $c\mid b$, then $c$ is a
    \emph{common divisor} of $a$ and $b$. 
    If at least one of the two integers $a$ and $b$ is non-zero,
    then there exists a \emph{greatest common divisor} of $a$ and
    $b$. It is denoted by \emph{$\gcd(a,b)$}. 
    

    How do we compute the greatest common divisor efficiently? The
    following is called \emph{division with remainder}.  For
    $a,b \in \setZ$
    with $b >0$ there exist unique integers $q,r \in \setZ$ with
    \begin{displaymath}
      a = q\cdot b + r, \text{ and }  0\leq r <b. 
    \end{displaymath}
    Now clearly, 
    for $a,b \in \setZ$ with $b >0$ and $q,r \in \setZ$ as above one has
    $\gcd(a,b) = \gcd(b,r)$.  
  This gives rise to the famous \emph{Euclidean algorithm}. 
  \begin{algorithm}[Euclidean algorithm]
    \label{alg:1}
    \begin{tabbing}
      Input: Integers $a ≥ b ≥0$ not both equal to zero \\
      Output: The greatest common divisor $\gcd(a,b)$\\
      {\bf if}  $(b=0)$ {\bf return} $a$ \\
      {\bf else} \= \\
      \> Compute $q,r ∈ ℕ$ with \=  $b > r ≥ 0$ and $a = q⋅b +r$ \\
      \> \> (division with remainder)\\
      \> {\bf return} $\gcd(b,r)$ 
    \end{tabbing}
  \end{algorithm}  
%
  \begin{theorem}
    The Euclidean algorithm runs in time $O(n)$. 
  \end{theorem}
  \begin{proof}
    Suppose that $a$ and $b$ have at most $n$ bits each.  Clearly, the
    numbers in the course of the algorithm have at most $n$
    bits. Furthermore, if $a \geq b$, then $r \leq a/2$, where $r$ is
    the remainder of the division of $a$ by $b$. Thus each second
    iteration, the first parameter of the input has one bit less. Thus
    the number of operations is bounded by $O(n)$.
  \end{proof}
  
\begin{example}
\label{exe:det}
The \emph{determinant} of a matrix $A ∈ ℝ^{n ×n}$ can be computed by the recursive formula 
\begin{displaymath}
  \det(A) = ∑_{i=1}^n (-1)^{1+j}a_{1j} \det(A_{1j}),
\end{displaymath}
where $A_{1j}$ is the $(n-1)×(n-1)$ matrix that is obtained from $A$ by deleting its  first row and $j$-th column.  This yields the following algorithm. 

\begin{tabbing}
  Input: $A ∈ ℚ^{n  ×n}$ \\
  Output: $\det(A)$ \\
  
  {\bf if} \= $(n=1)$ \\
           \> {\bf return} $a_{11}$ \\
  {\bf else} \\
           \> $d:=0$  \\
           \> {\bf for } \= $j=1,\dots,n$ \\
           \>            \> $d:= (-1)^{1+j}⋅ \det(A_{1j}) +d$\\
           \> {\bf return} $d$   
\end{tabbing}
Let $T(n)$ denote the number of basic operations that the algorithm performs. Then $T(1) ≥1$ and 
\begin{displaymath}
  T(n) ≥ n ⋅ T(n-1), 
\end{displaymath}
which shows that $T(n) >= n! = 2^{\Omega({n \log n})}$ which is \emph{exponential} in $n$. 
\end{example}



\section*{Exercises} 


\begin{enumerate}
\item Find the binary representation of $134$. 
\item Show that the binary representation with leading bit one of a positive natural number is
 unique. \label{alg:ex2}
\item Show that there are $n$-bit numbers $a,b ∈ ℕ$ such that the  Euclidean algorithm on input $a$ and $b$ performs  $\Omega(n)$ arithmetic operations. \emph{Hint: Fibonacci numbers} 
\item Show $n! =  2^{\Omega({n \log n})}$. 
\item Suppose that $B$ is a matrix that can be obtained from $A$ by deleting the first $k$ rows and some of the $k$ columns of $A$ and suppose that the $n^2$ components of $A$ are pairwise different. How many (recursive) calls of the form $\det(B)$ does the algorithm of Example~\ref{exe:det} create? 
\end{enumerate}

\section{Analysis of Gaussian elimination} 
\label{sec:analys-gauss-elim}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "lecture"
%%% End:
